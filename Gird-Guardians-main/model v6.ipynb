{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c04a92-bffd-4076-8a1e-6630f4c4bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c8ac2e-4924-430d-82ab-73da1ff96b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data (from Modelv5.ipynb)\n",
    "def simulate_data_with_interactions(n_samples=10000, noise_level=0.1, balance_factor=-1.735):\n",
    "    np.random.seed(42)\n",
    "    voltage = np.random.normal(230, 10, n_samples)\n",
    "    current = np.random.normal(50, 5, n_samples)\n",
    "    temperature = np.random.normal(40, 5, n_samples)\n",
    "    load = np.random.normal(70, 10, n_samples)\n",
    "    time_since_maintenance = np.random.exponential(100, n_samples)\n",
    "    moisture_level = np.random.normal(30, 5, n_samples)\n",
    "    lightning_surge = np.random.binomial(1, 0.1, n_samples)\n",
    "    voltage += np.random.normal(0, noise_level * 10, n_samples)\n",
    "    current += np.random.normal(0, noise_level * 5, n_samples)\n",
    "    temperature += np.random.normal(0, noise_level * 5, n_samples)\n",
    "    load += np.random.normal(0, noise_level * 10, n_samples)\n",
    "    time_since_maintenance += np.random.normal(0, noise_level * 10, n_samples)\n",
    "    moisture_level += np.random.normal(0, noise_level * 5, n_samples)\n",
    "    coef_voltage = 0.05\n",
    "    coef_current = 0.1\n",
    "    coef_temperature = 0.4\n",
    "    coef_load = 0.05\n",
    "    coef_time = 0.02\n",
    "    coef_moisture = 0.3\n",
    "    coef_surge = 0.5\n",
    "    linear_comb = (\n",
    "        coef_voltage * (voltage - 230) +\n",
    "        coef_current * (current - 50) +\n",
    "        coef_temperature * (temperature - 40) +\n",
    "        coef_load * (load - 70) +\n",
    "        coef_time * (time_since_maintenance - 100) +\n",
    "        coef_moisture * (moisture_level - 30) +\n",
    "        coef_surge * lightning_surge + balance_factor\n",
    "    )\n",
    "    prob_failure = 1 / (1 + np.exp(-linear_comb))\n",
    "    failure = np.random.binomial(1, prob_failure)\n",
    "    df = pd.DataFrame({\n",
    "        'voltage': voltage,\n",
    "        'current': current,\n",
    "        'temperature': temperature,\n",
    "        'load': load,\n",
    "        'time_since_maintenance': time_since_maintenance,\n",
    "        'moisture_level': moisture_level,\n",
    "        'lightning_surge': lightning_surge,\n",
    "        'failure': failure\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e795abeb-6328-49ac-87ad-d4afa41608d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated failure rate: 31.07%\n",
      "Important features saved to important_features.txt: ['temperature moisture_level' 'time_since_maintenance moisture_level'\n",
      " 'voltage temperature' 'temperature' 'temperature time_since_maintenance'\n",
      " 'current time_since_maintenance' 'current temperature'\n",
      " 'voltage time_since_maintenance' 'load time_since_maintenance'\n",
      " 'voltage moisture_level']\n",
      "\n",
      "Class distribution after SMOTETomek:\n",
      "failure\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Random Forest best params: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 15, 'class_weight': 'balanced'}\n",
      "XGBoost best params: {'subsample': 0.6, 'scale_pos_weight': 5, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "SVM best params: {'gamma': 0.1, 'class_weight': 'balanced', 'C': 1}\n",
      "Ensemble model saved to grid_maintenance_ensemble_model.pkl\n",
      "\n",
      "Transformer Failure Predictions:\n",
      "Transformer 1 - Status: Failure Not Present\n",
      "Transformer 2 - Status: Failure Present\n",
      "Transformer 3 - Status: Failure Present\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Paths\n",
    "    MODEL_PATH = 'grid_maintenance_ensemble_model.pkl'\n",
    "    POLY_PATH = 'poly_transformer.pkl'\n",
    "    SCALER_PATH = 'scaler.pkl'\n",
    "    FEATURES_PATH = 'important_features.txt'\n",
    "\n",
    "    # Generate data\n",
    "    df = simulate_data_with_interactions()\n",
    "    print(f\"Simulated failure rate: {df['failure'].mean():.2%}\")\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = df.drop('failure', axis=1)\n",
    "    y = df['failure']\n",
    "\n",
    "    # Feature engineering\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    poly_feature_names = poly.get_feature_names_out(X.columns)\n",
    "    X = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X_scaled, columns=poly_feature_names)\n",
    "\n",
    "    # Save transformers\n",
    "    joblib.dump(poly, POLY_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "    # Feature selection\n",
    "    rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(X, y)\n",
    "    important_features = pd.DataFrame({\n",
    "        'Feature': poly_feature_names,\n",
    "        'Importance': rf_temp.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False).head(10)['Feature'].values\n",
    "    X = X[important_features]\n",
    "\n",
    "    # Save important features\n",
    "    with open(FEATURES_PATH, 'w') as f:\n",
    "        f.write('\\n'.join(important_features))\n",
    "    print(f\"Important features saved to {FEATURES_PATH}: {important_features}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    # Apply SMOTETomek\n",
    "    smotetomek = SMOTETomek(random_state=42)\n",
    "    X_train_res, y_train_res = smotetomek.fit_resample(X_train, y_train)\n",
    "    print(\"\\nClass distribution after SMOTETomek:\")\n",
    "    print(pd.Series(y_train_res).value_counts(normalize=True))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    # Random Forest\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 5],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 5}]\n",
    "    }\n",
    "    rf_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, n_iter=10, cv=5, scoring='f1', random_state=42)\n",
    "    rf_search.fit(X_train_res, y_train_res)\n",
    "    rf_model = rf_search.best_estimator_\n",
    "    print(f\"Random Forest best params: {rf_search.best_params_}\")\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 7, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.6, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.8],\n",
    "        'scale_pos_weight': [1, 5]\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(XGBClassifier(random_state=42), xgb_param_grid, n_iter=10, cv=5, scoring='f1', random_state=42)\n",
    "    xgb_search.fit(X_train_res, y_train_res)\n",
    "    xgb_model = xgb_search.best_estimator_\n",
    "    print(f\"XGBoost best params: {xgb_search.best_params_}\")\n",
    "\n",
    "    # SVM (optimized for faster tuning)\n",
    "    svm_param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto', 0.1],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 5}]\n",
    "    }\n",
    "    svm_search = RandomizedSearchCV(SVC(probability=True, random_state=42), svm_param_grid, n_iter=5, cv=3, scoring='f1', random_state=42)\n",
    "    svm_search.fit(X_train_res, y_train_res)\n",
    "    svm_model = svm_search.best_estimator_\n",
    "    print(f\"SVM best params: {svm_search.best_params_}\")\n",
    "\n",
    "    # Ensemble\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_model),\n",
    "            ('xgb', xgb_model),\n",
    "            ('svm', svm_model)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        weights=[1, 1, 1.2]\n",
    "    )\n",
    "    ensemble_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(ensemble_model, MODEL_PATH)\n",
    "    print(f\"Ensemble model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Test inference\n",
    "    new_data = pd.DataFrame({\n",
    "        'voltage': [230, 235, 228],\n",
    "        'current': [50, 52, 49],\n",
    "        'temperature': [40, 45, 42],\n",
    "        'load': [70, 75, 68],\n",
    "        'time_since_maintenance': [100, 150, 120],\n",
    "        'moisture_level': [30, 35, 32],\n",
    "        'lightning_surge': [0, 1, 0]\n",
    "    })\n",
    "    try:\n",
    "        # Apply polynomial features\n",
    "        X_poly = poly.transform(new_data)\n",
    "        poly_feature_names = poly.get_feature_names_out(new_data.columns)\n",
    "        X = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "        # Scale all polynomial features\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=poly_feature_names)\n",
    "        # Select important features\n",
    "        X = X_scaled[important_features]\n",
    "        # Predict\n",
    "        probabilities = ensemble_model.predict_proba(X)[:, 1]\n",
    "        status = [\"Failure Present\" if prob >= 0.2 else \"Failure Not Present\" for prob in probabilities]\n",
    "        print(\"\\nTransformer Failure Predictions:\")\n",
    "        for i, stat in enumerate(status):\n",
    "            print(f\"Transformer {i+1} - Status: {stat}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558629ad-0bdc-4517-84cd-509c062feec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: joblib\n",
      "Version: 1.4.2\n",
      "Summary: Lightweight pipelining with Python functions\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
      "License: BSD 3-Clause\n",
      "Location: C:\\Users\\AK\\AppData\\Roaming\\Python\\Python312\\site-packages\n",
      "Requires: \n",
      "Required-by: imbalanced-learn, nltk, scikit-learn\n"
     ]
    }
   ],
   "source": [
    "!pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad745ede-b315-4687-847a-d8cb8be7f093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
